---
title: "ZOZOテクノロジーズ新卒採用 1次課題"
author: "安藤光希 (Koki Ando)"
date: '`r Sys.Date()`'
output:
  html_document:
    code_folding: hide
    highlight: tango
    number_section: yes
    theme: cosmo
    toc: yes
    toc_depth: 3
  pdf_document:
    toc: yes
    toc_depth: '3'
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
getwd()
```

*1次課題: ファッションECにおけるレコメンドエンジン*

# 設問

> ファッションECの特徴をうまく利用したエンジンを作る想定で以下の問いに答えてください。

1. ファッションアイテムを扱うECで観測される「購買ログ」にはどんな特性が存在すると思いますか？
商材ごとにEC上におけるユーザーの購買行動は変化すると考えることは自然な発想です。観測されるデータの背後で起こっていると思われる現象と絡めて説明してください。
本問はレコメンドの経験を問うものではありません。自由に想像して回答してください。
(参考までに、一般的なECで得られるデータのサンプルを提示します。これのほか、実際のECで扱っていると思われるデータやSNSなど外部のデータソースと組み合わせて回答しても構いません。)

2. どんなアルゴリズムが有効だと思いますか？
1.で説明した特性を利用したレコメンドアルゴリズムを開発し、ECの売上向上を目指します。
どんなアルゴリズムが有効だと思いますか？アルゴリズムの概要および効果的だと考える理由を説明して下さい。

3. 2で説明したレコメンドアルゴリズムを実装してください。
一般に公開されているベンチマーク、もしくは自作したダミーデータを用いてアルゴリズムの稼働確認をして下さい。ただし、稼働確認に用いるデータはファッションに関係していなくても構いませんし、1.で説明した特性を満たしている必要もありません。
(本問は目的に対して手段を正しく選択できていることを重視するため、高度なアルゴリズムである必要はありません。目的に対して手段を正しく選択できるかを重視するため高度なアルゴリズムである必要はない。)

# 解答

## 1. ファッションアイテムを扱うEC上での購買ログにはどのような特性が存在するのか？

+ ユーザーごとの属性によって大きく異なる。
+ ユーザーが同時に購買する商品にはパターンが存在する。

大きく分けて上記に二点が存在すると考える。

### ユーザーごとの属性による購買ログの違い

ここで意味する属性の違いというのは、性別や年齢といった大きなカテゴリーで分類するだけではない。特にファッションECサイトを利用するユーザーは、日本国内だけでも、居住地域の天候や人口構成などによって大きく個人ユーザーが必要としているアイテムも大きく変わると考えられる。中でも自分は「気候」に注目できると考える。南北に長い日本の国土だけ捉えたとしても、北と南では一年間の平均気温には大きな差があるし、西と南では降水量や湿度にも大きく変化が確率が高い。そのため、ファッションEC上の購買ログの特性には大きく居住地(商品の配送先)などが大きなファクターであると考えた。

### 購買商品の組み合わせ

二つ目にあげたファッションECサイト上の購買ログの特性には、ユーザーの購買商品のパターンが大きく関連されていると考えた。当設問の回答を考える際に自分は普段どのようにECサイトを利用しているか、なぜECを利用しているのかを考えた。

自分は通常**一つ以上目当ての商品を思い浮かべてからECサイトを訪れている**。ユーザーの中には、ウィンドーショッピング目的で訪れ、そこで気になった商品があると購買するというユーザーも多数いると考えられるが、自分はそのケースは当てはまらない。そのため、今回の分析レポートの分析対象からはそのようなユーザーを外すことにした。

購買商品のパターンというのは具体的に、特定のブランドを好んで多く買っていたり、メインの商品と共に、靴や手袋といったアクセサリー商品を同時に購買するといったパターンが想像できる。

自分の場合、必ず一つの求めていた商品を見つけた後に、関連商品やその他セール対象になっている商品なども探し、複数個の商品をカートにプールして決済を行う。
自分のこのようなECサイト上での行動経験をもとに以下の特徴が大きくあると仮定した。
ユーザーは最も優先順位の高い商品を買うのみではなく、それに関連した商品も探し、同時に決済することが多い。

以上の二点が自分が考えた、ファッションEC上での購買ログの特性である。

## どんなアルゴリズムが有効か？

アルゴリズムは二つに大きく分類してK平均法クラスタリングという教師無し機械学習と、マーケットバスケット分析(以降MBAと表記する)という伝統的にマーケティングでしようされているアルゴリズムを用いる。

### K平均法クラスター分析

主に、ユーザーを特徴的なデータで分類するために、クラスタリングを用いる。理由としてはクラスター分析は、数あるデータを一つずつ区分してグルーピングする有効なアルゴリズムであるためである。

### マーケットバスケット分析(MBA)

> Market Basket Analysisは商品同士の関連性を明らかにする為に小売大手によって用いられている重要な技術の一つである。主に、商品決済において頻繁に同時に取引される商品の組み合わせを明らかにすることによって当分析手法は効果を発揮する。言うなれば、当手法は売り手側が商品売り上げの関連性を明確に理解させることを可能にする手法なのである。(筆者日本語訳)
> Market Basket Analysis is one of the key techniques used by large retailers to uncover associations between items. It works by looking for combinations of items that occur together frequently in transactions. To put it another way, it allows retailers to identify relationships between the items that people buy.
[R-bloggers](https://www.r-bloggers.com/a-gentle-introduction-on-market-basket-analysis%E2%80%8A-%E2%80%8Aassociation-rules/)より引用

以上が、オンライン文献より引用した説明であるが、このレポートではこのマーケットバスケット分析(以降MBAと記述する)手法を用いて、主にどの商品が同時に購入されることが多いか、を発見する。このアルゴリズムを利用する理由としては、ユーザーの購買ログから頻出する商品組み合わせを見つけるには非常に効率的であるためである。

## レコメンドアルゴリズムの実装

一般に公開されているベンチマーク、もしくは自作したダミーデータを用いてアルゴリズムの稼働確認をして下さい。ただし、稼働確認に用いるデータはファッションに関係していなくても構いませんし、1.で説明した特性を満たしている必要もありません。(本問は目的に対して手段を正しく選択できていることを重視するため、高度なアルゴリズムである必要はありません。目的に対して手段を正しく選択できるかを重視するため高度なアルゴリズムである必要はない。)

必要なパッケージの読込

```{r warning=FALSE, message=FALSE}
library(tidyverse) #データ整形や分析フローの効率化のため
library(data.table) #データの表示やいくつかの関数を使用するため
library(cluster) #クラスター分析全般で用いるため
```

### K平均法を用いたユーザーの振り分け

#### ダミーデータ  

ユーザーデータのダミーを作る際には[擬似個人情報データ生成サービス](https://hogehoge.tk/personal/)と[都道府県別年平均気温ランキング](http://grading.jpn.org/SRB02101.html)を利用した。
(データの作成と整形は下記スクリプトを参照。)

+ 外部リンク: [ダミーデータの作成スクリプト](https://github.com/koki25ando/insurance/blob/master/user_dummy.R)

```{r}
dummy_df <- read.csv("dummy_df.csv")
str(dummy_df)
```

データの中身は

+ 現住所
+ 氏名
+ 年齢
+ 居住地の年間平均気温

以上の四つが主に含まれている。

#### クラスター分析

```{r}
model <- kmeans(dummy_df %>%
                  select(年平均気温, 年齢), #居住地の気候とユーザーの年齢を抽出
                  center = 10) #クラスター数は10に設定
dummy_km10 <- mutate(dummy_df %>%
                      select(年平均気温, 年齢, 都道府県),
                      cluster = model$cluster) #上で作成したモデル内のクラスターIDを貼り付ける
```

```{r eval = FALSE}
dummy_km10 %>%
  ggplot(aes(x = 年齢, y = 年平均気温, colour = as.factor(cluster))) +
  geom_point() +
  stat_ellipse(show.legend = F) +
  theme_grey(base_family = "HiraKakuProN-W3")
```

![](/Users/KokiAndo/Desktop/R/R report/Insurance/cluster_plot.jpeg)

クラスター数は便宜上10個とした。理由としては、そもそもデータが実データではなく恣意的に整形されたものでありなおかつ、ファッションECにおいては年齢や居住地域以外の要素が大きく影響されると考えたため、アルゴリズムをシンプル化するために今回は10クラスターで行なった。

上記で作ったデータのクラスター内のダミーユーザー数には偏りがあるため、その配分に応じてのちに購買データのトランザクションもランダムに配分する。

### マーケットバスケット分析

[Bread Basket data](https://github.com/viktree/curly-octo-chainsaw/blob/master/BreadBasket_DMS.csv)
スコットランドのBread Basket社が以前提供していたデータをもとに分析を行う。

実際のECの購買ログデータを使用するのが、望ましかったが残念ながら公開されているものは見つけられなかった。そのため、一取引ごとにユーザーとユーザーの購買商品が記録されている[Bread Basket data](https://github.com/viktree/curly-octo-chainsaw/blob/master/BreadBasket_DMS.csv)を用いる。

```{r}
transactions_df <- read.csv("BreadBasket_DMS.csv")
str(transactions_df)
```

匿名顧客の2016年10月30日から2017年4月9日までの購買データ。データには9684回分の取引データがあり、

+ 購買日時
+ 取引ID
+ 購入商品

主に以上の三つが含まれている。

#### キーとなる分析指標を求める

このレポートでは、売上のパターンを発掘するためにこの統計手法を用いる。"cross-selling opportunities"を見つける。例)商品Aを購入した顧客は商品Bも共に購入する傾向がある。

+ Probability(A): 任意の商品Aが購入される確率 = 商品Aが買われた数/全取引数
+ Support(A,B): 任意の商品Aと商品Bのが同時に購入される確率 = 商品AとBが両方含まれた取引数/全取引数

##### Probability

```{r fig.height=8}
Probability_df <- data.frame(table(transactions_df$Item)) %>%
  mutate(Pro = round(Freq/sum(Freq), digits = 5))
# Probability_df %>%
#   ggplot(aes(x = Var1, y = (Freq/nrow(transactions_df)), fill = Var1)) +
#   geom_bar(stat = "identity", show.legend = FALSE) +
#   coord_flip() +
#   labs(x = "Item Name", y = "Frequency")
```

商品の購買頻度に大きな散らばりが見受けられる。そのため、今回の分析においては購買頻度が極端に少ない商品は排除する。具体的には、購買回数が5回以下のものは全て除外することにした。

```{r fig.height=8}
Probability <- Probability_df %>%
  filter(Freq > 5) #購入回数が5回以上のみの商品のみを選択する
Probability %>%
  ggplot(aes(x = Var1, y = (Freq/nrow(transactions_df)), fill = Var1)) +
  geom_bar(stat = "identity", show.legend = FALSE) +
  coord_flip() +
  labs(x = "Item Name", y = "Frequency")
```

##### Support

```{r}
Support <- read.csv("Support.csv")
```

主な指標の一つであるSupportを求めるために上で使っていた、transactions_dfを整形して、[Support.csv](https://github.com/koki25ando/insurance/blob/master/Support.csv)を作成した。
整形スクリプトは[こちら](https://github.com/koki25ando/insurance/blob/master/support.R)を参照。

```{r}
head(Support, 30)
```

このデータには主に

+ 購買商品のセット組み合わせ
+ 組み合わせの頻度
+ 全取引に対しての任意の組合せの割合

以上の三つのデータが含まれている。

```{r}
Result_pro_sup <- function(first_item){
  model = list()
  #第一指標Probabilityを抽出
   pro_of_first_item <- Probability %>%
    filter(Var1 == first_item)
   model$Probability_of_First_Item <- pro_of_first_item$Pro

   #第二指標Supportを算出する
   support_result <- Support[Support$Var1 %like% first_item, ] %>%
     # mutate(Confidence = Support/Pobability_of_item) %>%
     head(30)
   model$Support <- support_result

   Potential_items <- support_result$Var1 %>%
     str_remove_all(paste0(first_item, ", ")) %>%
     str_remove_all(paste0(", ", first_item)) %>%
     str_remove_all(first_item)
   model$Potentials <- Potential_items
   model
}
model <- Result_pro_sup(first_item="Sandwich")
```

##### レコメンド関数

```{r}
### クラスターを判別する関数
detect_cluster <- function(Address, Age){
  temperature <- dummy_km10 %>%
    filter(都道府県 == Address) %>% #インプットされた都道府県を使って、年間平均気温を導く
    select(年平均気温)
  avr_temp <- temperature[1,] #ユーザーの居住地の年間平均気温の変数を作成

  similar_points <- dummy_km10 %>%
    filter(年平均気温 > avr_temp-1, 年平均気温 < avr_temp+1, 年齢 > Age-3, 年齢 < Age+3) %>% #年間平均気温+-1,年齢の+-3の幅で抽出
    select(cluster)
  cluster_num = names(which.max(table(similar_points))) #クラスターナンバーの最頻値を取得

  paste0("あなたが属するクラスターは第", cluster_num, "クラスターです。") %>%
    print()
  plot = dummy_km10 %>%
    ggplot() +
    geom_point(aes(x = 年齢, y = 年平均気温, colour = as.factor(cluster))) +
    # stat_ellipse(show.legend = F) +
    theme_grey(base_family = "HiraKakuProN-W3") +
    geom_point(aes(x=Age, y = avr_temp), colour = "red", size = 3) +
    labs(title = "赤い点があなたです。")
  print(plot)
}

### 過去の購買ログを分析して、おすすめアイテムを見つける関数
recommend_item <- function(first_item){
  model <- Result_pro_sup(first_item)

   model$Support$Var1 <- model$Support$Var1 %>%
    str_remove_all(paste0(first_item, ", ")) %>%
    str_remove_all(paste0(", ", first_item)) %>%
    str_remove(first_item)
    Confidence <- model$Support %>%
      filter(Var1 != "") %>%
      select(Var1, Freq) %>%
      mutate(Total = sum(Freq)) %>%
      group_by(Var1) %>%
      mutate(Sub_Total = sum(Freq),
             Ratio = Sub_Total/Total) %>%
      select(Var1, Ratio) %>%
      ungroup()
  recommend_df <- Confidence[!duplicated(Confidence$Var1), ] %>%
    arrange(desc(Ratio))

  for (i in 1:5){
    print(paste0(first_item, "を購入した人は", recommend_df[i,]$Var1, "を",
                 round(recommend_df[i,]$Ratio*100, digits = 2), "%の確率で買ってます。"))
  }
}


# 上記二つの関数をまとめる最終的な関数
recommend_me <- function(Address, Age, first_item){
  detect_cluster(Address, Age)
  recommend_item(first_item)
}
```


## レコメンドエンジンの動作確認

### 概要

![](/Users/KokiAndo/Desktop/R/R report/Insurance/RE_general.png)

上記の図が主なレコメンドエンジンのワークフローである。

以下より、ユーザーの出身地と年齢、購買商品を仮想データとして設定して、レコメンドエンジンの動作確認を行う。

### 例１

+ 居住地: 大阪府
+ 年齢: 50歳
+ 最初の購買商品: Muffin


```{r}
recommend_me(Address = "大阪府", Age = 50, first_item = "Muffin")
```

### 例2

+ 居住地: 北海道
+ 年齢: 22歳
+ 最初の購買商品: Coffee


```{r}
recommend_me(Address = "北海道", Age = 22, first_item = "Coffee")
```

### 例3

+ 居住地: 鹿児島県
+ 年齢: 35歳
+ 最初の購買商品: Sandwich


```{r}
recommend_me(Address = "鹿児島県", Age = 35, first_item = "Sandwich")
```

上記では三つのレコメンドエンジン動作の例を提示した。

# Limitations

+ 使用したデータは食品でありさらには、保存が効かない商品についてだった。その為季節性の激しいさらには長持ちする衣類を対象にすると思うような効果を出せない可能性もある。
+ さらには一般的に衣類はブランド品から日用品といったものまで幅広い価格帯の商品が多く、さらには季節性も激しいと思われる。

以上が今回のレコメンドエンジンの問題として考えられることである。

# 改善点

レポートのまとめとして、ダミーデータではなく実際のデータを用いる際のレコメンドエンジン構築について二つの主な昨日に分けて改善できる箇所を挙げる。

## ユーザークラスタリング

今回はダミーデータを用いたため、居住地の気候と年齢のみを指標に二次元データをクラスタリングした。しかし、最初の自分の解答でも述べたように重要な要素は多数あり、実際にはこのようなシンプルな属性のみでユーザーを区分けし、最適な商品をおすすめすることなどできないであろう。
実際のデータはより多くの因子をもとにレコメンド対象者を選別することができ、MBAもより効果的に機能することができるであろう。さらに、今回のレコメンドエンジンでは、ユーザーが属するクラスターまでは判別することができるが、そこから踏み込んだアルゴリズムは組んでいないため、より豊富なデータがあればその点も十分改善されると思われる。

## MBA分析

今回はダミーデータとして、入手が容易であった食品の購買データを利用した。しかし、根本的に食品とファッションアイテムには違いがある。

+ 商品によっての価格帯が大きく変動するので、ユーザーごとにクラスタリングしたのちに、クラスターごとの購買ログから上述したパターン認識を行い商品をレコメンドするといった手法も実行可能ではありそうである。実際のデータがあれば是非とも挑戦したいと思った。
+ このレポートで用いたように扱う商品が多くなればなるほどなかなか組み合わせというものは見つけづらいかも知れない。その為、サンプルデータで提供されていたような**小カテゴリやブランドといった要因**で同時に購入される商品の組み合わせを発掘して、レコメンドするという方法も可能であると思われる。
+ さらに、ファッションアイテムは季節性が出やすいため、期間ごとの購買ログなどに分けてレコメンドエンジンを実装すると、より精度の高いものを作ることができ、ユーザーの満足度も増やすことができると思われる。


以上。

安藤光希

# 参考文献リスト

+ [Association Mining with R](http://r-statistics.co/Association-Mining-With-R.html)
+ [Association Rules and Market Basket Analysis with R](https://www.r-bloggers.com/association-rules-and-market-basket-analysis-with-r/)
+ [Cluster Analysis of Whisky Review Dataset](https://www.kaggle.com/koki25ando/cluster-analysis-of-whisky-reviews-using-k-means)
+ [Analyzing Interesting Patterns from Large Transaction Data Sets](http://www.jmlr.org/papers/volume12/hahsler11a/hahsler11a.pdf)
+ [STITCH FIX Tech Blog](https://multithreaded.stitchfix.com/blog/)
+ [レコメンドの研究者、Julian McAuleyのサイト](https://cseweb.ucsd.edu/~jmcauley/)
+ [国際会議 ECCV 2018 workshop 採択論文](https://sites.google.com/view/eccvfashion/papers)
+ [国際会議 ICCV 2017 workshop Computer Vision For Fashion](https://sites.google.com/zalando.de/cvf-iccv2017/)
